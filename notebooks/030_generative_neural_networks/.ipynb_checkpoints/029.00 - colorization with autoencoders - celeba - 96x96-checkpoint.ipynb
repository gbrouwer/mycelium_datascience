{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorization with autoencoders - celeba 96x96\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import PIL\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from matplotlib import font_manager as fm, rcParams\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csfont = {'fontname':'Georgia'}\n",
    "hfont = {'fontname':'Helvetica'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../../data/celeba/colorization/'\n",
    "imgs_dir = '../../data/celeba/colorization/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "filenames = []\n",
    "for root, dirs, files in os.walk(\"../../data/celeba/pickle/\", topdown=False):\n",
    "    for name in files:\n",
    "        if '.p' in name:\n",
    "            filenames.append(os.path.join(root, name))\n",
    "filenames = np.sort(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5024,  165, 5050, ..., 9570, 3717, 1217])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalist = []\n",
    "ycombined = []\n",
    "for i in range(2):\n",
    "    data = pickle.load( open(filenames[i],\"rb\"))\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    y = list(y['person_id'].values)\n",
    "    datalist.append(X)\n",
    "datatuple = tuple(datalist)\n",
    "X = np.concatenate(datatuple)\n",
    "\n",
    "\n",
    "y\n",
    "# x_train = data['x_train']\n",
    "# y_train = data['y_train']\n",
    "# x_test = data['x_test'] \n",
    "# y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CIFAR10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"../../data/cifar10/pickle/data.p\", \"rb\"))\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test'] \n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = x_train.shape[1]\n",
    "img_cols = x_train.shape[2]\n",
    "channels = x_train.shape[3]\n",
    "imgs = x_test[:100]\n",
    "imgs = imgs.reshape((10, 10, img_rows, img_cols, channels))\n",
    "imgs_orig = np.vstack([np.hstack(i) for i in imgs])\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis('off')\n",
    "plt.title('Test color images (Ground  Truth)',**csfont,fontsize=16)\n",
    "plt.imshow(imgs_orig, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to gray and plot again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gray = rgb2gray(x_train)\n",
    "x_test_gray = rgb2gray(x_test)\n",
    "imgs = x_test_gray[:100]\n",
    "imgs = imgs.reshape((10, 10, img_rows, img_cols))\n",
    "imgs_gray = np.vstack([np.hstack(i) for i in imgs])\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis('off')\n",
    "plt.title('Test gray images (Input)',**csfont,fontsize=16)\n",
    "plt.imshow(imgs_gray, interpolation='none', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_train_gray = x_train_gray.astype('float32') / 255\n",
    "x_test_gray = x_test_gray.astype('float32') / 255\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n",
    "x_train_gray = x_train_gray.reshape(x_train_gray.shape[0], img_rows, img_cols, 1)\n",
    "x_test_gray = x_test_gray.reshape(x_test_gray.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, 1)\n",
    "batch_size = 32\n",
    "kernel_size = 3\n",
    "latent_dim = 256\n",
    "layer_filters = [64, 128, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "shape = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "latent = Dense(latent_dim, name='latent_vector')(x)\n",
    "encoder = Model(inputs, latent, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(shape[1]*shape[2]*shape[3])(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        strides=2,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "outputs = Conv2DTranspose(filters=channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='sigmoid',\n",
    "                          padding='same',\n",
    "                          name='decoder_output')(x)\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine into single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Checkpoints, Learning Rate Adjustments, Optimizer, Loss Function and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../../data/cifar10/colorization/saved_models'\n",
    "model_name = 'colorized_ae_model.{epoch:03d}.h5'\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               verbose=1,\n",
    "                               min_lr=0.5e-6)\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "callbacks = [lr_reducer, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train_gray,\n",
    "                x_train,\n",
    "                validation_data=(x_test_gray, x_test),\n",
    "                epochs=30,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=callbacks)\n",
    "x_decoded = autoencoder.predict(x_test_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = x_decoded[:100]\n",
    "imgs = imgs.reshape((10, 10, img_rows, img_cols, channels))\n",
    "imgs_colorized = np.vstack([np.hstack(i) for i in imgs])\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis('off')\n",
    "plt.title('Colorized test images (Predicted)')\n",
    "plt.imshow(imgs_colorized, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
